{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0774fa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok!\n"
     ]
    }
   ],
   "source": [
    "print(\"ok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a2279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain.vectorstores import pinecone\n",
    "# import pinecone \n",
    "# from pinecone import Pinecone\n",
    "# from langchain_pinecone import PineconeVectorStore  # Make sure to install this package\n",
    "from langchain_community.vectorstores import Pinecone as LangchainPinecone\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789448c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = \"pcsk_2Utdae_AHvuS9XVfHBxGxKVW3R7scQenoL8Lb1B86bJNqSJBa2F2FiujnjyLhw7pPs3qJu\"\n",
    "PINECONE_API_ENV = \"us-east-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42efdd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract data from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10210e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(\n",
    "        data,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fc97e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df7e888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85934670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create text chunks\n",
    "def text_split(extracted_data):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)\n",
    "  text_chunks = text_splitter.split_documents(extracted_data)\n",
    "\n",
    "  return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4738323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of my chunk: 5860\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_split(extracted_data)\n",
    "print(\"length of my chunk:\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581fc361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8d773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39662b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fouzia\\AppData\\Local\\Temp\\ipykernel_13592\\1337643473.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\fouzia\\anaconda3\\envs\\mchatbot\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe5eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3bc0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33441053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d5355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 637 documents\n",
      "Created 5860 text chunks\n",
      "Embedding dimension: 384\n",
      "Successfully added 5860 chunks to Pinecone index 'chatbot'\n",
      "\n",
      "Query: What is this document about?\n",
      "Top 3 results:\n",
      "1. er will be corrected in future editions.\n",
      "This book is printed on recycled paper that meets Environme...\n",
      "2. er will be corrected in future editions.\n",
      "This book is printed on recycled paper that meets Environme...\n",
      "3. Bejel\n",
      "GEM -0433 to 0624 - B  10/22/03 6:08 PM  Page 460...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "\n",
    "# Import the correct Pinecone modules\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Define your Pinecone API key and index name\n",
    "PINECONE_API_KEY = \"pcsk_2Utdae_AHvuS9XVfHBxGxKVW3R7scQenoL8Lb1B86bJNqSJBa2F2FiujnjyLhw7pPs3qJu\"\n",
    "INDEX_NAME = \"chatbot\"\n",
    "\n",
    "# Function to load PDF documents\n",
    "def load_pdf(data_path):\n",
    "    loader = DirectoryLoader(\n",
    "        data_path,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "# Function to split text into chunks\n",
    "def text_split(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(documents)\n",
    "    return text_chunks\n",
    "\n",
    "# Function to download and initialize HuggingFace embeddings\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load PDF documents\n",
    "    extracted_data = load_pdf(\"data/\")\n",
    "    print(f\"Loaded {len(extracted_data)} documents\")\n",
    "    \n",
    "    # Split text into chunks\n",
    "    text_chunks = text_split(extracted_data)\n",
    "    print(f\"Created {len(text_chunks)} text chunks\")\n",
    "    \n",
    "    # Initialize embeddings\n",
    "    embeddings = download_hugging_face_embeddings()\n",
    "    \n",
    "    # Test embeddings\n",
    "    test_query = \"Hello world\"\n",
    "    query_result = embeddings.embed_query(test_query)\n",
    "    print(f\"Embedding dimension: {len(query_result)}\")\n",
    "    \n",
    "    # Initialize Pinecone client\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    \n",
    "    # Check if index exists, if not create it\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "    \n",
    "    if INDEX_NAME not in existing_indexes:\n",
    "        # Create index if it doesn't exist\n",
    "        print(f\"Creating new index: {INDEX_NAME}\")\n",
    "        pc.create_index(\n",
    "            name=INDEX_NAME,\n",
    "            dimension=len(query_result),  # Use the dimension from our test embedding\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\"\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Connect to the index\n",
    "    index = pc.Index(INDEX_NAME)\n",
    "    \n",
    "    # Create vector store\n",
    "    vector_store = PineconeVectorStore(\n",
    "        index=index,\n",
    "        embedding=embeddings,\n",
    "        text_key=\"text\"\n",
    "    )\n",
    "    \n",
    "    # Prepare texts and metadata\n",
    "    texts = [chunk.page_content for chunk in text_chunks]\n",
    "    metadatas = [chunk.metadata for chunk in text_chunks]\n",
    "    \n",
    "    # Add texts to vector store\n",
    "    vector_store.add_texts(texts=texts, metadatas=metadatas)\n",
    "    \n",
    "    print(f\"Successfully added {len(texts)} chunks to Pinecone index '{INDEX_NAME}'\")\n",
    "    \n",
    "    # Test a query\n",
    "    query = \"What is this document about?\"\n",
    "    results = vector_store.similarity_search(query, k=3)\n",
    "    \n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"Top 3 results:\")\n",
    "    for i, doc in enumerate(results):\n",
    "        print(f\"{i+1}. {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7f2726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      "Purpose\n",
      "Allergy is a reaction of the immune system. Nor-\n",
      "mally, the immune system responds to foreign microor-\n",
      "ganisms and particles, like pollen or dust, by producing\n",
      "specific proteins called antibodies that are capable of\n",
      "binding to identifying molecules, or antigens, on the\n",
      "foreign organisms. This reaction between antibody and\n",
      "antigen sets off a series of reactions designed to protect\n",
      "the body from infection. Sometimes, this same series of\n",
      "\n",
      "\n",
      "Result 2:\n",
      "Purpose\n",
      "Allergy is a reaction of the immune system. Nor-\n",
      "mally, the immune system responds to foreign microor-\n",
      "ganisms and particles, like pollen or dust, by producing\n",
      "specific proteins called antibodies that are capable of\n",
      "binding to identifying molecules, or antigens, on the\n",
      "foreign organisms. This reaction between antibody and\n",
      "antigen sets off a series of reactions designed to protect\n",
      "the body from infection. Sometimes, this same series of\n",
      "\n",
      "\n",
      "Result 3:\n",
      "reaction. Allergic rhinitis is characterized by an itchy,\n",
      "runny nose, often with a scratchy or irritated throat due\n",
      "to post-nasal drip. Inflammation of the thin membrane\n",
      "covering the eye (allergic conjunctivitis) causes redness,\n",
      "irritation, and increased tearing in the eyes. Asthma caus-\n",
      "es wheezing, coughing, and shortness of breath. Symp-\n",
      "toms of food allergies depend on the tissues most sensi-\n",
      "tive to the allergen and whether the allergen spread sys-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "# If we already have an index we can load it like this\n",
    "from pinecone import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "PINECONE_API_KEY = \"pcsk_2Utdae_AHvuS9XVfHBxGxKVW3R7scQenoL8Lb1B86bJNqSJBa2F2FiujnjyLhw7pPs3qJu\"\n",
    "# <-- Replace this with your actual key\n",
    "INDEX_NAME = \"chatbot\"  # <-- Replace with your actual index name\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Connect to Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "# Create vector store\n",
    "docsearch = PineconeVectorStore(\n",
    "    index=index,\n",
    "    embedding=embeddings,\n",
    "    text_key=\"text\"\n",
    ")\n",
    "\n",
    "# Perform query\n",
    "query = \"What are Allergies\"\n",
    "docs = docsearch.similarity_search(query, k=3)\n",
    "\n",
    "# Pretty print results\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\nResult {i+1}:\\n{doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e9a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Use the following context to answer the question.\n",
    "If you don't know the answer, just say you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d576e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import CTransformers\n",
    "\n",
    "llm = CTransformers(\n",
    "    model=\"model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "    model_type=\"llama\",\n",
    "    config={\n",
    "        'max_new_tokens': 512,\n",
    "        'temperature': 0.8\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c52d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Create the retrieval QA chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k': 2}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd8578b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fouzia\\AppData\\Local\\Temp\\ipykernel_1612\\3253210397.py:5: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  result = qa({\"query\": user_input})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  \n",
      "A) is caused by a virus\n",
      "B) can only be contracted through sexual contact\n",
      "C) is transmitted through direct contact with an infected person's blood\n",
      "D) can be spread through the air\n",
      "E) occurs only in certain parts of the world\n"
     ]
    }
   ],
   "source": [
    "# while True:\n",
    "#     user_input = input(\"Input Prompt: \")\n",
    "#     if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "#         break\n",
    "#     result = qa({\"query\": user_input})\n",
    "#     print(\"Response: \", result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03711eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ok!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mchatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
